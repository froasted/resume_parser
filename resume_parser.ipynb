{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "#using the nltk library\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[H\u001B[2J"
     ]
    }
   ],
   "source": [
    "!pdf2txt.py sample_resume.pdf > resume.txt\n",
    "file_text = (open(\"sample_resume.txt\", \"r\")).read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #use for doc file\n",
    "# import docx2txt\n",
    "# file_text = docx2txt.process(\"sample_resume.docx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding email id using regex parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def email_extractor(file_text):\n",
    "    email_id = re.findall(\"[\\w\\.-]+@[\\w\\.-]+\", file_text)\n",
    "    return email_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ayan1910@gmail.com']\n"
     ]
    }
   ],
   "source": [
    "print(email_extractor(file_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding phone numbers using regex parsing \n",
    "##### (pattern matches for +91-9304398005 +919304398005 09304398005 9304398005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phone_number_extractor(file_text):\n",
    "    phone_number = re.findall(\"\\+\\d+(?:\\-)\\d{10}|\\+\\d{12}|\\\\b0\\d{10}|\\d{10}\", file_text)\n",
    "    return phone_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['09304398005']\n"
     ]
    }
   ],
   "source": [
    "print(phone_number_extractor(file_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(text_string): \n",
    "    \n",
    "    #lower casting \n",
    "    file_text_lowered = text_string.lower()\n",
    "    \n",
    "    #removing unwanted punctuations\n",
    "    punctuations = '''!()-[]{};:'\"—@%–\\,\\.<>•◦/?#$^&*_~'''\n",
    "  \n",
    "    for x in file_text_lowered: \n",
    "        if x in punctuations: \n",
    "            file_text_lowered = file_text_lowered.replace(x, \"\")\n",
    "            \n",
    "    #Tokenizing        \n",
    "    tokenized_words = word_tokenize(file_text_lowered)\n",
    "    \n",
    "    #Removing stop words\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    filtered_sentence = [w for w in tokenized_words if not w in stop_words]\n",
    "    \n",
    "    return filtered_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_doc = data_preprocessing(file_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ayan', 'chatterjee', 'member', 'technical', 'staff', 'il', 'profile', 'details', 'aspiring', 'data', 'scientist', 'skilled', 'python', 'apache', 'spark', 'hadoop', 'someone', 'loves', 'analyzing', 'tweaking', 'data', 'sets', 'bangalore', 'india', '09304398005', 'ayan1910gmailcom', 'date', 'birth', '19101995', 'nationality', 'indian', 'languages', 'english', 'hindi', 'bengali', 'hobbies', 'singing', 'badminton', 'cricket', 'gaming', 'employment', 'history', 'member', 'technical', 'staff', 'il', 'nineleaps', 'bangalore', 'march', '2020', 'present', 'client', 'multinational', 'general', 'groceries', 'merchandise', 'retailer', 'main', 'aspect', 'project', 'forecast', 'demand', 'product', 'store', 'followed', 'rulebased', 'approach', 'get', 'optimum', 'profit', 'soon', 'possible', 'used', 'big', 'data', 'technologies', 'like', 'pyspark', 'oozie', 'sqoop', 'get', 'data', 'relational', 'databases', 'hdfs', 'perform', 'rule', 'based', 'approach', 'also', 'awarded', 'employee', 'month', 'march', 'debugging', 'major', 'loopholes', 'project', 'graduate', 'engineer', 'trainee', 'nineleaps', 'bangalore', 'january', '2019', 'march', '2020', 'internship', 'period', 'nineleaps', 'introduced', 'pyspark', 'distributed', 'storage', 'technologies', 'involving', 'h', 'dfs', 'databases', 'including', 'teradata', 'large', 'part', 'work', 'involved', 'studying', 'various', 'projects', 'based', 'spark', 'understanding', 'spark', 'fundamentals', 'architecture', 'resource', 'managers', 'internal', 'working', 'technical', 'skills', 'programming', 'languages', 'python', 'numpy', 'beginner', 'automation', 'frameworks', 'jenkins', 'apache', 'oozie', 'databases', 'teradata', 'visualization', 'tools', 'excel', 'big', 'data', 'technologies', 'pyspark', 'hbase', 'hive', 'apache', 'sqoop', 'others', 'bash', 'scripting', 'data', 'analyzing', 'education', 'electronics', 'communication', 'engineering', 'acharya', 'institutes', 'bangalore', 'july', '2015', 'july', '2019', 'isc', 'hill', 'top', 'school', 'jamshedpur', 'march', '2012', 'march', '2014', 'secured', '8025', 'pcm', 'participated', 'inter', 'intraschool', 'singing', 'competitions', 'isce', 'hill', 'top', 'school', 'jamshedpur', 'march', '2012', 'march', '2014', 'secured', '916', 'pcm', 'participated', 'debates', 'singing', 'competitions', 'courses', 'neural', 'networks', 'deep', 'learning', 'coursera', 'april', '2020', 'present', 'machine', 'learning', 'coursera', 'january', '2020', 'present']\n"
     ]
    }
   ],
   "source": [
    "print(processed_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Skill Sets using sample data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Used pre-built file containing various skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "skills_test_set = pd.read_csv('skills.csv')\n",
    "skills_list = skills_test_set['skill_name'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skills_extractor(processed_doc):\n",
    "    full_match=[]\n",
    "    partial_match=[]\n",
    "    \n",
    "    for i in skills_list:\n",
    "        for j in processed_doc:\n",
    "            if j == i and j not in full_match:\n",
    "                full_match.append(j)\n",
    "            \n",
    "    for i in skills_list:\n",
    "        for j in processed_doc:\n",
    "            if j in i and j not in full_match and j not in partial_match:\n",
    "                partial_match.append(j)\n",
    "                \n",
    "    print(\"Full skill matches are {0} {1}\".format(full_match, '\\n'))\n",
    "    print(\"Partial skill matches are {0}\".format(partial_match))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full skill matches are ['python', 'hadoop', 'singing', 'bash', 'automation', 'scripting', 'english', 'visualization', 'sqoop', 'hbase', 'electronics', 'hindi', 'history', 'jenkins', 'bengali', 'pyspark'] \n",
      "\n",
      "Partial skill matches are ['excel', 'h', 'data', 'programming', 'project', 'apache', 'spark', 'learning', 'machine', 'numpy', 'il', 'inter', 'big', 'engineer', 'engineering', 'relational', 'databases', 'product', 'deep', 'technical', 'skills', 'work', 'neural', 'networks', 'resource', 'main', 'working', 'forecast', 'distributed', 'hive', 'internal', 'top', 'perform', 'get', 'architecture', 'present', 'debugging', 'general', 'store', 'demand', 'isc', 'communication', 'education', 'fundamentals', 'ayan', 'employment', 'storage', 'aspect', 'tools']\n"
     ]
    }
   ],
   "source": [
    "skills_extractor(processed_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Names based on first 10 words on a resume (Debatable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import enchant ## Using pyenchant's dictionary to elimate english words from being considered as a Name\n",
    "d = enchant.Dict(\"en_US\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing_with_lowering(text_string): \n",
    "    \n",
    "    file_text = text_string\n",
    "    \n",
    "    #removing unwanted punctuations\n",
    "    punctuations = '''!()-[]{};:'\"—@%–\\,\\.<>•◦/?#$^&*_~'''\n",
    "  \n",
    "    for x in file_text: \n",
    "        if x in punctuations: \n",
    "            file_text = file_text.replace(x, \"\")\n",
    "            \n",
    "    #Tokenizing        \n",
    "    tokenized_words = word_tokenize(file_text)\n",
    "    \n",
    "    #Removing stop words\n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    filtered_sentence = [w for w in tokenized_words if not w in stop_words]\n",
    "    \n",
    "    return filtered_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_processed_doc = data_preprocessing_with_lowering(file_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using regex to filter Names having length more than 3 (debatable)\n",
    "def extract_name(doc):\n",
    "    for i in range(len(doc[:10])):\n",
    "        if d.check(doc[i]) is False and re.search(\"\\w{3,}\", doc[i]) is not None:\n",
    "            print(doc[i], end =\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ayan Chatterjee "
     ]
    }
   ],
   "source": [
    "extract_name(new_processed_doc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}